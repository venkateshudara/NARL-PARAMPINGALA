HPC cluster with peak computing power of 1.2PF or more with interconnect and software and 
2 Petabytes Parallel File System (PFS) with Tape Library

1. Compute Nodes 
 
The CPU only Compute Nodes are based on Intel FC2HAC16W3 with S9248WK2HAC Compute Module (2U modular system with 2 independent hot-plug nodes inside which share only the enclosure and the power supplies (therefore 1 unit of Intel FC2HAC16W3 with S9248WK2HAC will provide 2 nodes) - 
 
Number of Systems – 98x Intel FC2HAC16W3 with S9248WK2HAC Compute Module (196 Nodes)- Configuration of each Node - ➢ Dual Intel® Xeon® Platinum 9242 Processor (48C, 2.3G) ➢ 768GB using 24 x 32GB modules of DDR4-2933 ECC RDIMM ➢ 1x Intel® Omni-Path Host Fabric Interface Adapter 100 Series ➢ Remote Management Module for Out of Band Management 
 
2. Master Nodes 
 
Number of Systems – 2x Intel Server System R2208WFTZSR Configuration of each Node - ➢ Dual Intel® Xeon® Gold 6240 Processor (24.75M Cache, 2.60 GHz) ➢ 384GB using 12 x 32GB modules of DDR4-2933 ECC RDIMM ➢ 2x 600GB SAS 10K HDD ➢ Shared storage Array with Redundant Controller populated with 8x 600GB SAS 10K HDD ➢ DVD-R/W Drive ➢ SAS RAID Card  ➢ 1G Interface for Connectivity to Secondary Communication Network ➢ 10G Interface for Storage Connectivity ➢ 1x Intel® Omni-Path Host Fabric Interface Adapter 100 Series ➢ Remote Management Module for Out of Band Management ➢ Second Power Supply for Redundancy 
 
3. Login/Management/Boot Nodes 
 
Number of Systems – 14x Intel Server System R2208WFTZSR Configuration of each Node - ➢ Dual Intel® Xeon® Gold 6240 Processor (24.75M Cache, 2.60 GHz) ➢ 384GB using 12 x 32GB modules of DDR4-2933 ECC RDIMM ➢ 6x 600GB SAS 10K HDD ➢ DVD-R/W Drive ➢ SAS RAID Card  ➢ 1G Interface for Connectivity to Secondary Communication Network ➢ 1x Intel® Omni-Path Host Fabric Interface Adapter 100 Series ➢ Remote Management Module for Out of Band Management ➢ Second Power Supply for Redundancy 
